支持向量机（SVM），一个神秘而众知的名字，在其出来就受到了莫大的追捧，号称最优秀的分类算法之一，
以其简单的理论构造了复杂的算法，又以其简单的用法实现了复杂的问题。

分隔超平面：上述将数据集分割开来的直线叫做分隔超平面。
超平面：如果数据集是N维的，那么就需要N-1维的某对象来对数据进行分割。该对象叫做超平面，也就是分类的决策边界。
间隔：一个点到分割面的距离，称为点相对于分割面的距离。数据集中所有的点到分割面的最小间隔的2倍，称为分类器或数据集的间隔。
最大间隔：SVM分类器是要找最大的数据集间隔。
支持向量：离分割超平面最近的那些点


对于非线性可切分的数据集，要做分割，就要借助于核函数了简单一点说呢，核函数可以看做对原始特征的一个映射函数， 不过SVM不会傻乎乎对原始样本点做映射，它有更巧妙的方式来保证这个过程的高效性。


支持向量机如何用于自然语言分类？

有了这个算法，我们就可以在多维空间中对向量进行分类了。如何将它引入文本分类任务呢？首先你要做的就是把文本的片断整合为一个数字向量，这样才能使用 SVM 进行区分。换句话说，什么属性需要被拿来用作 SVM 分类的特征呢？

最常见的答案是字频，就像在朴素贝叶斯中所做的一样。这意味着把文本看作是一个词袋，对于词袋中的每个单词都存在一个特征，特征值就是这个词出现的频率。

这样，问题就被简化为：这个单词出现了多少次，并把这个数字除以总字数。


一些现实世界中 SVM 在其他领域里的应用或许会用到数十，甚至数百个特征值。
同时自然语言处理分类用到了数千个特征值，在最坏的情况下，每个词都只在训练集中出现过一次。
这会让问题稍有改变：非线性核心或许在其他情况下很好用，但特征值过多的情况下可能会造成非线性核心数据过拟合。
因此，最好坚持使用旧的线性核心，这样才能在那些例子中获得很好的结果。

核函数技巧实际上并不是 SVM 的一部分。它可以与其他线性分类器共同使用，如逻辑回归等。支持向量机只负责找到决策边界。